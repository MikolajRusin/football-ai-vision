{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is responsible for downloading and preparing datasets from various source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> 5 videos from google drive (which will be split into frames)  \n",
    "---> dataset from roboflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define path of the project and the paths where the train/valid/test sets will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\n",
      "|\n",
      "|--data -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\n",
      "|  |--images -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\train\n",
      "|  |   |--train -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\train\n",
      "|  |   |--valid -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\valid\n",
      "|  |   |--test -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\test\n",
      "|  |   \n",
      "|  |--origin_videos -> C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Path of the project\n",
    "PROJECT_PATH = Path(os.getcwd()).resolve().parent\n",
    "\n",
    "# Make the main data dir\n",
    "DATA_PATH = PROJECT_PATH / 'data'\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Make the dir where train/valid/test set will be saved\n",
    "IMAGES_PATH = DATA_PATH / 'images'\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# Make the train dir\n",
    "TRAIN_PATH = IMAGES_PATH / 'train'\n",
    "os.makedirs(TRAIN_PATH, exist_ok=True)\n",
    "\n",
    "# Make the valid dir\n",
    "VALID_PATH = IMAGES_PATH / 'valid'\n",
    "os.makedirs(VALID_PATH, exist_ok=True)\n",
    "\n",
    "# Make the test dir\n",
    "TEST_PATH = IMAGES_PATH / 'test'\n",
    "os.makedirs(TEST_PATH, exist_ok=True)\n",
    "\n",
    "# Make the dir where the original videos will be stored\n",
    "ORIGIN_VIDEOS_PATH = DATA_PATH / 'origin_videos'\n",
    "os.makedirs(ORIGIN_VIDEOS_PATH, exist_ok=True)\n",
    "\n",
    "print(f'Project path -> {PROJECT_PATH}')\n",
    "print(f'|')\n",
    "print(f'|--data -> {DATA_PATH}')\n",
    "print(f'|  |--images -> {TRAIN_PATH}')\n",
    "print(f'|  |   |--train -> {TRAIN_PATH}')\n",
    "print(f'|  |   |--valid -> {VALID_PATH}')\n",
    "print(f'|  |   |--test -> {TEST_PATH}')\n",
    "print(f'|  |   ')\n",
    "print(f'|  |--origin_videos -> {ORIGIN_VIDEOS_PATH}')\n",
    "print(f'|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing videos from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12TqauVZ9tLAv8kWxTTBFWtgt2hNQ4_ZF\n",
      "To: C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\\0bfacc_0.mp4\n",
      "\n",
      "  0%|          | 0.00/19.9M [00:00<?, ?B/s]\n",
      "  3%|▎         | 524k/19.9M [00:00<00:03, 5.03MB/s]\n",
      " 13%|█▎        | 2.62M/19.9M [00:00<00:01, 13.8MB/s]\n",
      " 37%|███▋      | 7.34M/19.9M [00:00<00:00, 28.6MB/s]\n",
      " 53%|█████▎    | 10.5M/19.9M [00:00<00:00, 23.9MB/s]\n",
      " 74%|███████▍  | 14.7M/19.9M [00:00<00:00, 26.0MB/s]\n",
      "100%|██████████| 19.9M/19.9M [00:00<00:00, 30.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19PGw55V8aA6GZu5-Aac5_9mCy3fNxmEf\n",
      "To: C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\\2e57b9_0.mp4\n",
      "\n",
      "  0%|          | 0.00/21.1M [00:00<?, ?B/s]\n",
      "  2%|▏         | 524k/21.1M [00:00<00:04, 4.79MB/s]\n",
      " 12%|█▏        | 2.62M/21.1M [00:00<00:01, 13.2MB/s]\n",
      " 37%|███▋      | 7.86M/21.1M [00:00<00:00, 28.7MB/s]\n",
      " 52%|█████▏    | 11.0M/21.1M [00:00<00:00, 25.2MB/s]\n",
      " 70%|██████▉   | 14.7M/21.1M [00:00<00:00, 28.8MB/s]\n",
      " 97%|█████████▋| 20.4M/21.1M [00:00<00:00, 37.6MB/s]\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 30.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OG8K6wqUw9t7lp9ms1M48DxRhwTYciK-\n",
      "To: C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\\08fd33_0.mp4\n",
      "\n",
      "  0%|          | 0.00/19.9M [00:00<?, ?B/s]\n",
      "  3%|▎         | 524k/19.9M [00:00<00:05, 3.85MB/s]\n",
      " 13%|█▎        | 2.62M/19.9M [00:00<00:01, 12.0MB/s]\n",
      " 40%|███▉      | 7.86M/19.9M [00:00<00:00, 27.2MB/s]\n",
      " 55%|█████▌    | 11.0M/19.9M [00:00<00:00, 24.7MB/s]\n",
      " 74%|███████▍  | 14.7M/19.9M [00:00<00:00, 28.3MB/s]\n",
      "100%|██████████| 19.9M/19.9M [00:00<00:00, 29.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1yYPKuXbHsCxqjA9G-S6aeR2Kcnos8RPU\n",
      "To: C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\\573e61_0.mp4\n",
      "\n",
      "  0%|          | 0.00/18.9M [00:00<?, ?B/s]\n",
      "  3%|▎         | 524k/18.9M [00:00<00:04, 4.28MB/s]\n",
      " 14%|█▍        | 2.62M/18.9M [00:00<00:01, 12.6MB/s]\n",
      " 36%|███▌      | 6.82M/18.9M [00:00<00:00, 18.2MB/s]\n",
      " 58%|█████▊    | 11.0M/18.9M [00:01<00:00, 9.69MB/s]\n",
      " 69%|██████▉   | 13.1M/18.9M [00:01<00:00, 10.4MB/s]\n",
      " 92%|█████████▏| 17.3M/18.9M [00:01<00:00, 15.4MB/s]\n",
      "100%|██████████| 18.9M/18.9M [00:01<00:00, 14.0MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vVwjW1dE1drIdd4ZSILfbCGPD4weoNiu\n",
      "To: C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\origin_videos\\121364_0.mp4\n",
      "\n",
      "  0%|          | 0.00/17.2M [00:00<?, ?B/s]\n",
      "  3%|▎         | 524k/17.2M [00:00<00:04, 3.70MB/s]\n",
      " 15%|█▌        | 2.62M/17.2M [00:00<00:01, 12.2MB/s]\n",
      " 46%|████▌     | 7.86M/17.2M [00:00<00:00, 27.4MB/s]\n",
      " 64%|██████▍   | 11.0M/17.2M [00:00<00:00, 23.4MB/s]\n",
      " 97%|█████████▋| 16.8M/17.2M [00:00<00:00, 33.1MB/s]\n",
      "100%|██████████| 17.2M/17.2M [00:00<00:00, 27.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown -O \"{ORIGIN_VIDEOS_PATH}/0bfacc_0.mp4\" \"https://drive.google.com/uc?id=12TqauVZ9tLAv8kWxTTBFWtgt2hNQ4_ZF\"\n",
    "!gdown -O \"{ORIGIN_VIDEOS_PATH}/2e57b9_0.mp4\" \"https://drive.google.com/uc?id=19PGw55V8aA6GZu5-Aac5_9mCy3fNxmEf\"\n",
    "!gdown -O \"{ORIGIN_VIDEOS_PATH}/08fd33_0.mp4\" \"https://drive.google.com/uc?id=1OG8K6wqUw9t7lp9ms1M48DxRhwTYciK-\"\n",
    "!gdown -O \"{ORIGIN_VIDEOS_PATH}/573e61_0.mp4\" \"https://drive.google.com/uc?id=1yYPKuXbHsCxqjA9G-S6aeR2Kcnos8RPU\"\n",
    "!gdown -O \"{ORIGIN_VIDEOS_PATH}/121364_0.mp4\" \"https://drive.google.com/uc?id=1vVwjW1dE1drIdd4ZSILfbCGPD4weoNiu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from videos and move them to appropriate directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting frames from the videos will involve going through it at a certain frequency of frame extraction. The frame extraction frequency is defineed in the variable STRIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each frame will be saved as .jpg format and will have a unique index, for example: image_0341.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "# ----------------------------------------------------EXTRACTING FRAMES FROM VIDEOS----------------------------------------------------\n",
    "def extract_frames(video_path: Path, output_path: Path, start_idx: int=0, stride = 5):\n",
    "    video_info = sv.VideoInfo.from_video_path(video_path)\n",
    "    frame_generator = sv.get_video_frames_generator(video_path, stride=stride)\n",
    "\n",
    "    frame_idx = start_idx\n",
    "    current_video = str(video_path).split('\\\\')[-1]\n",
    "    for frame in tqdm(frame_generator, desc=f'Extracting frames from video -> {current_video}', total=int(video_info.total_frames / 5)):\n",
    "        output_frame_path = output_path / f'image_{frame_idx:04d}.jpg'\n",
    "        cv2.imwrite(output_frame_path, frame)\n",
    "\n",
    "        frame_idx += 1\n",
    "# ----------------------------------------------------EXTRACTING FRAMES FROM VIDEOS----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames from video -> 08fd33_0.mp4: 100%|██████████| 150/150 [00:06<00:00, 23.63it/s]\n",
      "Extracting frames from video -> 0bfacc_0.mp4: 100%|██████████| 150/150 [00:06<00:00, 24.10it/s]\n",
      "Extracting frames from video -> 121364_0.mp4: 100%|██████████| 150/150 [00:06<00:00, 24.01it/s]\n",
      "Extracting frames from video -> 2e57b9_0.mp4: 100%|██████████| 150/150 [00:06<00:00, 24.01it/s]\n",
      "Extracting frames from video -> 573e61_0.mp4: 100%|██████████| 150/150 [00:06<00:00, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 images from videos were saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the dir where the extracted frames will be saved\n",
    "EXTRACTED_FRAMES_DIR = 'extracted_frames'\n",
    "EXTRACTED_FRAMES_PATH = DATA_PATH / EXTRACTED_FRAMES_DIR\n",
    "os.makedirs(EXTRACTED_FRAMES_PATH, exist_ok=True)\n",
    "\n",
    "STRIDE = 5\n",
    "start_idx_videos = []\n",
    "for video_file_name in os.listdir(ORIGIN_VIDEOS_PATH):\n",
    "    video_path = ORIGIN_VIDEOS_PATH / video_file_name\n",
    "    start_idx = len(os.listdir(EXTRACTED_FRAMES_PATH))\n",
    "    start_idx_videos.append(start_idx)\n",
    "\n",
    "    extract_frames(video_path, EXTRACTED_FRAMES_PATH, start_idx, STRIDE)\n",
    "\n",
    "print(f'{len(os.listdir(EXTRACTED_FRAMES_PATH))} images from videos were saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 150, 300, 450, 600]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, valid and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_file_names = sorted(os.listdir(EXTRACTED_FRAMES_PATH))\n",
    "np_file_names = np.array(sorted_file_names)\n",
    "image_ids = np.arange(len(np_file_names))\n",
    "bins = start_idx_videos[1:]\n",
    "\n",
    "video_mask = np.digitize(image_ids, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 0: 150 images\n",
      "Video 1: 150 images\n",
      "Video 2: 150 images\n",
      "Video 3: 150 images\n",
      "Video 4: 150 images\n"
     ]
    }
   ],
   "source": [
    "video_ids, counts = np.unique(video_mask, return_counts=True)\n",
    "\n",
    "for n_video, count in zip(video_ids, counts):\n",
    "    print(f'Video {n_video}: {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALID_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "train_full_images, valid_images, train_full_mask, valid_mask = train_test_split(np_file_names, video_mask, test_size=VALID_SIZE, stratify=video_mask)\n",
    "\n",
    "train_images, test_images, train_mask, test_mask = train_test_split(train_full_images, train_full_mask, test_size=TEST_SIZE, stratify=train_full_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Video 0: 121 images\n",
      "Video 1: 122 images\n",
      "Video 2: 121 images\n",
      "Video 3: 121 images\n",
      "Video 4: 122 images\n",
      "Total Train Images: 607\n",
      "\n",
      "Valid Set\n",
      "Video 0: 15 images\n",
      "Video 1: 15 images\n",
      "Video 2: 15 images\n",
      "Video 3: 15 images\n",
      "Video 4: 15 images\n",
      "Total Valid Images: 75\n",
      "\n",
      "Test Set\n",
      "Video 0: 14 images\n",
      "Video 1: 13 images\n",
      "Video 2: 14 images\n",
      "Video 3: 14 images\n",
      "Video 4: 13 images\n",
      "Total Test Images: 68\n"
     ]
    }
   ],
   "source": [
    "print('Train Set')\n",
    "video_ids, train_counts = np.unique(train_mask, return_counts=True)\n",
    "for n_video, count in zip(video_ids, train_counts):\n",
    "    print(f'Video {n_video}: {count} images')\n",
    "print(f'Total Train Images: {len(train_mask)}')\n",
    "\n",
    "print('\\nValid Set')\n",
    "video_ids, valid_counts = np.unique(valid_mask, return_counts=True)\n",
    "for n_video, count in zip(video_ids, valid_counts):\n",
    "    print(f'Video {n_video}: {count} images')\n",
    "print(f'Total Valid Images: {len(valid_mask)}')\n",
    "\n",
    "print('\\nTest Set')\n",
    "video_ids, test_counts = np.unique(test_mask, return_counts=True)\n",
    "for n_video, count in zip(video_ids, test_counts):\n",
    "    print(f'Video {n_video}: {count} images')\n",
    "print(f'Total Test Images: {len(test_mask)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ----------------------------------------------------MOVING IMAGES TO APPROPRIATE FOLDER----------------------------------------------------\n",
    "def move_images(image_paths_list: list[Path], target_folder_path: Path):\n",
    "    for image_path in tqdm(image_paths_list, desc=f'Transferring images to {target_folder_path}', total=len(image_paths_list)):\n",
    "        if str(image_path).endswith('.jpg'):\n",
    "            output_image_path = target_folder_path / str(image_path).split('\\\\')[-1]\n",
    "\n",
    "            shutil.move(image_path, output_image_path)\n",
    "# ----------------------------------------------------MOVING IMAGES TO APPROPRIATE FOLDER----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\train: 100%|██████████| 607/607 [00:00<00:00, 1132.37it/s]\n",
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\valid: 100%|██████████| 75/75 [00:00<00:00, 1019.57it/s]\n",
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\test: 100%|██████████| 68/68 [00:00<00:00, 606.02it/s]\n"
     ]
    }
   ],
   "source": [
    "image_sets = [\n",
    "    [EXTRACTED_FRAMES_PATH / train_image for train_image in train_images],\n",
    "    [EXTRACTED_FRAMES_PATH / valid_image for valid_image in valid_images],\n",
    "    [EXTRACTED_FRAMES_PATH / test_image for test_image in test_images]\n",
    "]\n",
    "target_folder_sets = [TRAIN_PATH, VALID_PATH, TEST_PATH]\n",
    "\n",
    "for image_set, target_folder_set in zip(image_sets, target_folder_sets):\n",
    "    move_images(image_paths_list=image_set, target_folder_path=target_folder_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(EXTRACTED_FRAMES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in football-ai-vision-1 to coco:: 100%|██████████| 83381/83381 [00:02<00:00, 29269.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to football-ai-vision-1 in coco:: 100%|██████████| 380/380 [00:00<00:00, 1204.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from roboflow import Roboflow\n",
    "\n",
    "load_dotenv()\n",
    "ROBOFLOW_API_KEY = os.getenv('ROBOFLOW_API_KEY')\n",
    "\n",
    "HOME = os.getcwd()\n",
    "os.chdir(DATA_PATH)\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "project = rf.workspace(\"mikoaj-bu1z8\").project(\"football-ai-vision\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"coco\")\n",
    "os.chdir(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\train: 100%|██████████| 299/299 [00:00<00:00, 1056.56it/s]\n",
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\valid: 100%|██████████| 50/50 [00:00<00:00, 709.47it/s]\n",
      "Transferring images to C:\\Users\\miki0\\OneDrive\\Pulpit\\Projects\\my_projects\\FootballAI\\data\\images\\test: 100%|██████████| 26/26 [00:00<00:00, 1443.38it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'train'\n",
    "VALID_DIR = 'valid'\n",
    "TEST_DIR = 'test'\n",
    "\n",
    "image_sets = [\n",
    "    [Path(dataset.location) / TRAIN_DIR / train_image for train_image in os.listdir(Path(dataset.location) / TRAIN_DIR)],\n",
    "    [Path(dataset.location) / VALID_DIR / valid_image for valid_image in os.listdir(Path(dataset.location) / VALID_DIR)],\n",
    "    [Path(dataset.location) / TEST_DIR / test_image for test_image in os.listdir(Path(dataset.location) / TEST_DIR)]\n",
    "]\n",
    "target_folder_sets = [TRAIN_PATH, VALID_PATH, TEST_PATH]\n",
    "\n",
    "for image_set, target_folder_set in zip(image_sets, target_folder_sets):\n",
    "    move_images(image_paths_list=image_set, target_folder_path=target_folder_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(dataset.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
